{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wm1bocEqo8C"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Import required libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import re\n",
        "import string\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure GPU settings\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Enable memory growth to prevent TensorFlow from allocating all GPU memory\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"✓ {len(gpus)} GPU(s) detected and configured\")\n",
        "        print(f\"GPU Details: {gpus}\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "else:\n",
        "    print(\"✗ No GPU detected - will run on CPU\")\n",
        "\n",
        "# Verify CUDA\n",
        "print(f\"TensorFlow built with CUDA: {tf.test.is_built_with_cuda()}\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lRgqPzdxU41",
        "outputId": "44081265-72cb-45aa-e747-bf74edc9fed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ 1 GPU(s) detected and configured\n",
            "GPU Details: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "TensorFlow built with CUDA: True\n",
            "TensorFlow version: 2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Set hyperparameters and configuration\n",
        "BATCH_SIZE = 64\n",
        "TRAINING_EPOCHS = 100\n",
        "LSTM_UNITS = 256\n",
        "EMBED_SIZE = 128\n",
        "MAX_SAMPLES = 15000\n",
        "DATASET_PATH = \"hin.txt\"\n"
      ],
      "metadata": {
        "id": "YpO0DFMdqul5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Load and preprocess the dataset\n",
        "def clean_text(text):\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
        "    text = re.sub(r'[\" \"]+', \" \", text)\n",
        "    text = \"[start] \" + text + \" [end]\"\n",
        "    return text\n",
        "\n",
        "source_sentences = []\n",
        "target_sentences = []\n",
        "\n",
        "with open(DATASET_PATH, \"r\", encoding=\"utf-8\") as file:\n",
        "    data_lines = file.read().split(\"\\n\")\n",
        "\n",
        "for line in data_lines[: min(MAX_SAMPLES, len(data_lines) - 1)]:\n",
        "    split_parts = line.split(\"\\t\")\n",
        "    if len(split_parts) >= 2:\n",
        "        english_sentence, hindi_sentence = split_parts[0], split_parts[1]\n",
        "        source_sentences.append(hindi_sentence)\n",
        "        target_sentences.append(clean_text(english_sentence))\n",
        "\n",
        "# Split data into train and test sets (80-20 split)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_source, test_source, train_target, test_target = train_test_split(\n",
        "    source_sentences,\n",
        "    target_sentences,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Total samples: {len(source_sentences)}\")\n",
        "print(f\"Training samples: {len(train_source)}\")\n",
        "print(f\"Testing samples: {len(test_source)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eECQDFLqysu",
        "outputId": "d82be5f5-a577-42b2-aa24-d72266b9716c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 3116\n",
            "Training samples: 2492\n",
            "Testing samples: 624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Create text vectorization layers (using only training data)\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=5000,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=20\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=5000,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=20\n",
        ")\n",
        "\n",
        "# Adapt only on training data to prevent data leakage\n",
        "source_vectorization.adapt(train_source)\n",
        "target_vectorization.adapt(train_target)\n"
      ],
      "metadata": {
        "id": "26al5WU4qy7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Define custom text standardization and prepare dataset\n",
        "def standardize_text(text_input):\n",
        "    lowercase_text = tf.strings.lower(text_input)\n",
        "    # Remove all punctuation except square brackets\n",
        "    cleaned = tf.strings.regex_replace(\n",
        "        lowercase_text,\n",
        "        \"[!\\\"#$%&'()*+,-./:;<=>?@\\\\^_`{|}~]\",\n",
        "        \"\"\n",
        "    )\n",
        "    return cleaned\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=5000,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=20\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=5000,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=20,\n",
        "    standardize=standardize_text\n",
        ")\n",
        "\n",
        "# Adapt only on training data\n",
        "source_vectorization.adapt(train_source)\n",
        "target_vectorization.adapt(train_target)\n",
        "\n",
        "def prepare_training_batch(hindi, english):\n",
        "    hindi_encoded = source_vectorization(hindi)\n",
        "    english_encoded = target_vectorization(english)\n",
        "    return (\n",
        "        {\"encoder_inputs\": hindi_encoded, \"decoder_inputs\": english_encoded[:, :-1]},\n",
        "        english_encoded[:, 1:]\n",
        "    )\n",
        "\n",
        "# Create training dataset only from training split\n",
        "training_data = tf.data.Dataset.from_tensor_slices((train_source, train_target))\n",
        "training_data = training_data.batch(BATCH_SIZE).map(prepare_training_batch).shuffle(2048).prefetch(16)\n"
      ],
      "metadata": {
        "id": "8cH5tdmaqy9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Build the encoder-decoder architecture\n",
        "enc_input = keras.Input(shape=(None,), name=\"encoder_inputs\")\n",
        "enc_embedding = layers.Embedding(5000, EMBED_SIZE, mask_zero=True)(enc_input)\n",
        "\n",
        "enc_output, hidden_state, cell_state = layers.LSTM(\n",
        "    LSTM_UNITS,\n",
        "    return_state=True,\n",
        "    dropout=0.2\n",
        ")(enc_embedding)\n",
        "encoder_final_states = [hidden_state, cell_state]\n",
        "\n",
        "dec_input = keras.Input(shape=(None,), name=\"decoder_inputs\")\n",
        "dec_embedding = layers.Embedding(5000, EMBED_SIZE, mask_zero=True)(dec_input)\n",
        "decoder_lstm_layer = layers.LSTM(\n",
        "    LSTM_UNITS,\n",
        "    return_sequences=True,\n",
        "    return_state=True,\n",
        "    dropout=0.2\n",
        ")\n",
        "dec_output, _, _ = decoder_lstm_layer(dec_embedding, initial_state=encoder_final_states)\n",
        "output_layer = layers.Dense(5000, activation=\"softmax\")\n",
        "final_output = output_layer(dec_output)\n"
      ],
      "metadata": {
        "id": "ixRgjQRHqzAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Compile and train the model\n",
        "translation_model = keras.Model([enc_input, dec_input], final_output)\n",
        "translation_model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "translation_model.fit(training_data, epochs=TRAINING_EPOCHS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfhE0N9aqzCh",
        "outputId": "ae7c313f-62d9-4d06-daed-376dc6d892ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 29ms/step - accuracy: 0.2224 - loss: 7.7398\n",
            "Epoch 2/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4066 - loss: 5.1385\n",
            "Epoch 3/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1188 - loss: 4.7674\n",
            "Epoch 4/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1209 - loss: 4.6168\n",
            "Epoch 5/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.1231 - loss: 4.4800\n",
            "Epoch 6/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1248 - loss: 4.3926\n",
            "Epoch 7/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.1272 - loss: 4.2924\n",
            "Epoch 8/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.1302 - loss: 4.1878\n",
            "Epoch 9/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1321 - loss: 4.1219\n",
            "Epoch 10/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1333 - loss: 4.0726\n",
            "Epoch 11/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.1362 - loss: 3.9819\n",
            "Epoch 12/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1384 - loss: 3.9060\n",
            "Epoch 13/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1394 - loss: 3.8559\n",
            "Epoch 14/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1426 - loss: 3.7667\n",
            "Epoch 15/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1447 - loss: 3.6889\n",
            "Epoch 16/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1460 - loss: 3.6198\n",
            "Epoch 17/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.1480 - loss: 3.5379\n",
            "Epoch 18/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.1515 - loss: 3.4903\n",
            "Epoch 19/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.1540 - loss: 3.4235\n",
            "Epoch 20/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.1570 - loss: 3.3148\n",
            "Epoch 21/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1607 - loss: 3.2458\n",
            "Epoch 22/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1624 - loss: 3.1765\n",
            "Epoch 23/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1698 - loss: 3.1170\n",
            "Epoch 24/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1690 - loss: 3.0186\n",
            "Epoch 25/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1748 - loss: 2.9498\n",
            "Epoch 26/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1790 - loss: 2.8639\n",
            "Epoch 27/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1821 - loss: 2.8216\n",
            "Epoch 28/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.1871 - loss: 2.7407\n",
            "Epoch 29/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.1898 - loss: 2.6870\n",
            "Epoch 30/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.1940 - loss: 2.6338\n",
            "Epoch 31/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1983 - loss: 2.5811\n",
            "Epoch 32/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2019 - loss: 2.5036\n",
            "Epoch 33/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2066 - loss: 2.4528\n",
            "Epoch 34/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2144 - loss: 2.3437\n",
            "Epoch 35/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2163 - loss: 2.3229\n",
            "Epoch 36/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.2226 - loss: 2.2558\n",
            "Epoch 37/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.2280 - loss: 2.1743\n",
            "Epoch 38/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2306 - loss: 2.1281\n",
            "Epoch 39/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.2392 - loss: 2.0883\n",
            "Epoch 40/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.2442 - loss: 2.0086\n",
            "Epoch 41/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.2467 - loss: 1.9872\n",
            "Epoch 42/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2510 - loss: 1.9124\n",
            "Epoch 43/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2547 - loss: 1.8844\n",
            "Epoch 44/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2609 - loss: 1.8430\n",
            "Epoch 45/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2692 - loss: 1.7594\n",
            "Epoch 46/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2705 - loss: 1.7276\n",
            "Epoch 47/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2749 - loss: 1.6741\n",
            "Epoch 48/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2799 - loss: 1.6116\n",
            "Epoch 49/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2842 - loss: 1.5690\n",
            "Epoch 50/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.2899 - loss: 1.5314\n",
            "Epoch 51/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.2928 - loss: 1.4875\n",
            "Epoch 52/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.2970 - loss: 1.4569\n",
            "Epoch 53/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3020 - loss: 1.4022\n",
            "Epoch 54/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.3091 - loss: 1.3559\n",
            "Epoch 55/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3110 - loss: 1.3052\n",
            "Epoch 56/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3173 - loss: 1.2947\n",
            "Epoch 57/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3202 - loss: 1.2296\n",
            "Epoch 58/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3249 - loss: 1.1836\n",
            "Epoch 59/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3302 - loss: 1.1541\n",
            "Epoch 60/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3319 - loss: 1.0884\n",
            "Epoch 61/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3328 - loss: 1.0632\n",
            "Epoch 62/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.3381 - loss: 1.0433\n",
            "Epoch 63/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.3436 - loss: 1.0247\n",
            "Epoch 64/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3478 - loss: 0.9888\n",
            "Epoch 65/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3470 - loss: 0.9367\n",
            "Epoch 66/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3551 - loss: 0.9445\n",
            "Epoch 67/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3553 - loss: 0.8751\n",
            "Epoch 68/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3590 - loss: 0.8597\n",
            "Epoch 69/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3671 - loss: 0.8581\n",
            "Epoch 70/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3704 - loss: 0.8312\n",
            "Epoch 71/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3688 - loss: 0.7523\n",
            "Epoch 72/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3731 - loss: 0.7649\n",
            "Epoch 73/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.3764 - loss: 0.7400\n",
            "Epoch 74/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.3779 - loss: 0.7303\n",
            "Epoch 75/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3790 - loss: 0.6801\n",
            "Epoch 76/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3838 - loss: 0.6612\n",
            "Epoch 77/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3858 - loss: 0.6318\n",
            "Epoch 78/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3879 - loss: 0.6058\n",
            "Epoch 79/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3921 - loss: 0.5933\n",
            "Epoch 80/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3926 - loss: 0.5576\n",
            "Epoch 81/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3968 - loss: 0.5381\n",
            "Epoch 82/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3942 - loss: 0.5153\n",
            "Epoch 83/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3982 - loss: 0.5134\n",
            "Epoch 84/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.3983 - loss: 0.4795\n",
            "Epoch 85/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.4014 - loss: 0.4735\n",
            "Epoch 86/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.4031 - loss: 0.4539\n",
            "Epoch 87/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4058 - loss: 0.4370\n",
            "Epoch 88/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4076 - loss: 0.4086\n",
            "Epoch 89/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4096 - loss: 0.3953\n",
            "Epoch 90/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4114 - loss: 0.3757\n",
            "Epoch 91/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4105 - loss: 0.3623\n",
            "Epoch 92/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4135 - loss: 0.3541\n",
            "Epoch 93/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4204 - loss: 0.3543\n",
            "Epoch 94/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4166 - loss: 0.3314\n",
            "Epoch 95/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.4166 - loss: 0.3189\n",
            "Epoch 96/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.4180 - loss: 0.3079\n",
            "Epoch 97/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4185 - loss: 0.3049\n",
            "Epoch 98/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4234 - loss: 0.2889\n",
            "Epoch 99/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4217 - loss: 0.2716\n",
            "Epoch 100/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4223 - loss: 0.2547\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7dad2a6fd2e0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Build inference models for translation\n",
        "inference_encoder = keras.Model(enc_input, encoder_final_states)\n",
        "\n",
        "dec_hidden_input = keras.Input(shape=(LSTM_UNITS,))\n",
        "dec_cell_input = keras.Input(shape=(LSTM_UNITS,))\n",
        "decoder_state_inputs = [dec_hidden_input, dec_cell_input]\n",
        "\n",
        "dec_embedded = translation_model.layers[3](dec_input)\n",
        "dec_out, h_state, c_state = decoder_lstm_layer(\n",
        "    dec_embedded,\n",
        "    initial_state=decoder_state_inputs\n",
        ")\n",
        "decoder_final_states = [h_state, c_state]\n",
        "dec_predictions = output_layer(dec_out)\n",
        "inference_decoder = keras.Model(\n",
        "    [dec_input] + decoder_state_inputs,\n",
        "    [dec_predictions] + decoder_final_states\n",
        ")\n",
        "\n",
        "vocabulary = target_vectorization.get_vocabulary()\n",
        "\n",
        "def translate_sentence(hindi_input):\n",
        "    vectorized_input = source_vectorization([hindi_input])\n",
        "    encoder_states = inference_encoder.predict(vectorized_input, verbose=0)\n",
        "\n",
        "    decoder_input_seq = np.zeros((1, 1))\n",
        "    decoder_input_seq[0, 0] = vocabulary.index(\"[start]\")\n",
        "\n",
        "    should_stop = False\n",
        "    translated_text = \"\"\n",
        "\n",
        "    while not should_stop:\n",
        "        predictions, h, c = inference_decoder.predict(\n",
        "            [decoder_input_seq] + encoder_states,\n",
        "            verbose=0\n",
        "        )\n",
        "        predicted_token = np.argmax(predictions[0, -1, :])\n",
        "        predicted_word = vocabulary[predicted_token]\n",
        "\n",
        "        if predicted_word == \"[end]\" or len(translated_text.split()) > 20:\n",
        "            should_stop = True\n",
        "        else:\n",
        "            translated_text += \" \" + predicted_word\n",
        "            decoder_input_seq[0, 0] = predicted_token\n",
        "            encoder_states = [h, c]\n",
        "\n",
        "    return translated_text.strip()\n"
      ],
      "metadata": {
        "id": "bFXWtxLmqzEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Test the model with random samples from TEST set\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"MODEL TRANSLATION RESULTS (TEST SET)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for idx in range(10):\n",
        "    rand_idx = np.random.randint(0, len(test_source))\n",
        "    hindi_test = test_source[rand_idx]\n",
        "\n",
        "    predicted_translation = translate_sentence(hindi_test)\n",
        "\n",
        "    ground_truth = test_target[rand_idx].replace(\"[start]\", \"\").replace(\"[end]\", \"\").strip()\n",
        "\n",
        "    print(f\"\\nTest Case {idx + 1}:\")\n",
        "    print(f\" > Original (Hindi): {hindi_test}\")\n",
        "    print(f\" > Expected (English): {ground_truth}\")\n",
        "    print(f\" > Predicted (English): {predicted_translation}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL_sEYhyqzHA",
        "outputId": "7d2823af-ee89-479c-afb0-2cb1efc2614a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "MODEL TRANSLATION RESULTS (TEST SET)\n",
            "==================================================\n",
            "\n",
            "Test Case 1:\n",
            " > Original (Hindi): मेरे चाचा क्रिकेट के शौकिया खिलाड़ी हैं।\n",
            " > Expected (English): my uncle is an amateur cricket player .\n",
            " > Predicted (English): my uncle is an amateur cricket player\n",
            "\n",
            "Test Case 2:\n",
            " > Original (Hindi): मैं अगले सोमवार आके ले जाऊंगा।\n",
            " > Expected (English): i'll come pick it up next monday .\n",
            " > Predicted (English): ill be back at a bank\n",
            "\n",
            "Test Case 3:\n",
            " > Original (Hindi): उसका पर्स उससे चुरा लिया गया।\n",
            " > Expected (English): she was robbed of her purse .\n",
            " > Predicted (English): he went to india by her\n",
            "\n",
            "Test Case 4:\n",
            " > Original (Hindi): चीनी गर्म कॉफी में घुल जाती है।\n",
            " > Expected (English): sugar dissolves in hot coffee .\n",
            " > Predicted (English): your sister is very fond of music\n",
            "\n",
            "Test Case 5:\n",
            " > Original (Hindi): मैं अंग्रेज़ी पढ़ सकती हूँ।\n",
            " > Expected (English): i can read english .\n",
            " > Predicted (English): i will explain the manager\n",
            "\n",
            "Test Case 6:\n",
            " > Original (Hindi): उसका जन्मदिन इक्कीस अगस्त को है।\n",
            " > Expected (English): his birthday is august 21st .\n",
            " > Predicted (English): everybody likes playing tennis\n",
            "\n",
            "Test Case 7:\n",
            " > Original (Hindi): क्या तुम्हें गाड़ी चलाना आता है?\n",
            " > Expected (English): do you know how to drive a car ?\n",
            " > Predicted (English): do you know how to drive a car\n",
            "\n",
            "Test Case 8:\n",
            " > Original (Hindi): उसने केक को छः टुकड़ो में काट कर, हर बच्चे हो एक टुकड़ा दिया।\n",
            " > Expected (English): she cut the cake into six pieces and gave one to each of the children .\n",
            " > Predicted (English): she reproached me for not answering the letter\n",
            "\n",
            "Test Case 9:\n",
            " > Original (Hindi): उसका जन्मदिन इक्कीस अगस्त को है।\n",
            " > Expected (English): his birthday is august 21st .\n",
            " > Predicted (English): everybody likes playing tennis\n",
            "\n",
            "Test Case 10:\n",
            " > Original (Hindi): बोलो तुम्हें कौनसा चाहिए।\n",
            " > Expected (English): say which one you would like .\n",
            " > Predicted (English): you must be home\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Calculate and display test accuracy on TEST SET\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"COMPUTING TEST SET ACCURACY\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def calculate_word_accuracy(reference, hypothesis):\n",
        "    \"\"\"Calculate word-level accuracy between reference and hypothesis\"\"\"\n",
        "    ref_words = reference.lower().split()\n",
        "    hyp_words = hypothesis.lower().split()\n",
        "\n",
        "    if len(ref_words) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    correct_words = sum(1 for r, h in zip(ref_words, hyp_words) if r == h)\n",
        "    return correct_words / len(ref_words)\n",
        "\n",
        "# Evaluate on the entire test set (unseen data)\n",
        "test_sample_size = len(test_source)\n",
        "total_accuracy = 0.0\n",
        "perfect_matches = 0\n",
        "\n",
        "print(f\"\\nEvaluating on {test_sample_size} TEST samples (unseen during training)...\")\n",
        "\n",
        "for i in range(test_sample_size):\n",
        "    test_hindi = test_source[i]\n",
        "    expected_english = test_target[i].replace(\"[start]\", \"\").replace(\"[end]\", \"\").strip()\n",
        "    predicted_english = translate_sentence(test_hindi)\n",
        "\n",
        "    # Calculate word-level accuracy\n",
        "    accuracy = calculate_word_accuracy(expected_english, predicted_english)\n",
        "    total_accuracy += accuracy\n",
        "\n",
        "    # Count perfect translations\n",
        "    if predicted_english == expected_english:\n",
        "        perfect_matches += 1\n",
        "\n",
        "    # Show progress every 500 samples\n",
        "    if (i + 1) % 5 == 0:\n",
        "        print(f\"Processed {i + 1}/{test_sample_size} samples...\")\n",
        "\n",
        "average_word_accuracy = (total_accuracy / test_sample_size) * 100\n",
        "perfect_match_rate = (perfect_matches / test_sample_size) * 100\n",
        "\n",
        "print(f\"\\n{'─' * 50}\")\n",
        "print(f\"TEST SET RESULTS:\")\n",
        "print(f\"{'─' * 50}\")\n",
        "print(f\"Average Word-Level Accuracy: {average_word_accuracy:.2f}%\")\n",
        "print(f\"{'─' * 50}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZ6y3jKeqzJN",
        "outputId": "19038310-1407-41eb-a06a-bfda63921a82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "COMPUTING TEST SET ACCURACY\n",
            "==================================================\n",
            "\n",
            "Evaluating on 624 TEST samples (unseen during training)...\n",
            "Processed 5/624 samples...\n",
            "Processed 10/624 samples...\n",
            "Processed 15/624 samples...\n",
            "Processed 20/624 samples...\n",
            "Processed 25/624 samples...\n",
            "Processed 30/624 samples...\n",
            "Processed 35/624 samples...\n",
            "Processed 40/624 samples...\n",
            "Processed 45/624 samples...\n",
            "Processed 50/624 samples...\n",
            "Processed 55/624 samples...\n",
            "Processed 60/624 samples...\n",
            "Processed 65/624 samples...\n",
            "Processed 70/624 samples...\n",
            "Processed 75/624 samples...\n",
            "Processed 80/624 samples...\n",
            "Processed 85/624 samples...\n",
            "Processed 90/624 samples...\n",
            "Processed 95/624 samples...\n",
            "Processed 100/624 samples...\n",
            "Processed 105/624 samples...\n",
            "Processed 110/624 samples...\n",
            "Processed 115/624 samples...\n",
            "Processed 120/624 samples...\n",
            "Processed 125/624 samples...\n",
            "Processed 130/624 samples...\n",
            "Processed 135/624 samples...\n",
            "Processed 140/624 samples...\n",
            "Processed 145/624 samples...\n",
            "Processed 150/624 samples...\n",
            "Processed 155/624 samples...\n",
            "Processed 160/624 samples...\n",
            "Processed 165/624 samples...\n",
            "Processed 170/624 samples...\n",
            "Processed 175/624 samples...\n",
            "Processed 180/624 samples...\n",
            "Processed 185/624 samples...\n",
            "Processed 190/624 samples...\n",
            "Processed 195/624 samples...\n",
            "Processed 200/624 samples...\n",
            "Processed 205/624 samples...\n",
            "Processed 210/624 samples...\n",
            "Processed 215/624 samples...\n",
            "Processed 220/624 samples...\n",
            "Processed 225/624 samples...\n",
            "Processed 230/624 samples...\n",
            "Processed 235/624 samples...\n",
            "Processed 240/624 samples...\n",
            "Processed 245/624 samples...\n",
            "Processed 250/624 samples...\n",
            "Processed 255/624 samples...\n",
            "Processed 260/624 samples...\n",
            "Processed 265/624 samples...\n",
            "Processed 270/624 samples...\n",
            "Processed 275/624 samples...\n",
            "Processed 280/624 samples...\n",
            "Processed 285/624 samples...\n",
            "Processed 290/624 samples...\n",
            "Processed 295/624 samples...\n",
            "Processed 300/624 samples...\n",
            "Processed 305/624 samples...\n",
            "Processed 310/624 samples...\n",
            "Processed 315/624 samples...\n",
            "Processed 320/624 samples...\n",
            "Processed 325/624 samples...\n",
            "Processed 330/624 samples...\n",
            "Processed 335/624 samples...\n",
            "Processed 340/624 samples...\n",
            "Processed 345/624 samples...\n",
            "Processed 350/624 samples...\n",
            "Processed 355/624 samples...\n",
            "Processed 360/624 samples...\n",
            "Processed 365/624 samples...\n",
            "Processed 370/624 samples...\n",
            "Processed 375/624 samples...\n",
            "Processed 380/624 samples...\n",
            "Processed 385/624 samples...\n",
            "Processed 390/624 samples...\n",
            "Processed 395/624 samples...\n",
            "Processed 400/624 samples...\n",
            "Processed 405/624 samples...\n",
            "Processed 410/624 samples...\n",
            "Processed 415/624 samples...\n",
            "Processed 420/624 samples...\n",
            "Processed 425/624 samples...\n",
            "Processed 430/624 samples...\n",
            "Processed 435/624 samples...\n",
            "Processed 440/624 samples...\n",
            "Processed 445/624 samples...\n",
            "Processed 450/624 samples...\n",
            "Processed 455/624 samples...\n",
            "Processed 460/624 samples...\n",
            "Processed 465/624 samples...\n",
            "Processed 470/624 samples...\n",
            "Processed 475/624 samples...\n",
            "Processed 480/624 samples...\n",
            "Processed 485/624 samples...\n",
            "Processed 490/624 samples...\n",
            "Processed 495/624 samples...\n",
            "Processed 500/624 samples...\n",
            "Processed 505/624 samples...\n",
            "Processed 510/624 samples...\n",
            "Processed 515/624 samples...\n",
            "Processed 520/624 samples...\n",
            "Processed 525/624 samples...\n",
            "Processed 530/624 samples...\n",
            "Processed 535/624 samples...\n",
            "Processed 540/624 samples...\n",
            "Processed 545/624 samples...\n",
            "Processed 550/624 samples...\n",
            "Processed 555/624 samples...\n",
            "Processed 560/624 samples...\n",
            "Processed 565/624 samples...\n",
            "Processed 570/624 samples...\n",
            "Processed 575/624 samples...\n",
            "Processed 580/624 samples...\n",
            "Processed 585/624 samples...\n",
            "Processed 590/624 samples...\n",
            "Processed 595/624 samples...\n",
            "Processed 600/624 samples...\n",
            "Processed 605/624 samples...\n",
            "Processed 610/624 samples...\n",
            "Processed 615/624 samples...\n",
            "Processed 620/624 samples...\n",
            "\n",
            "──────────────────────────────────────────────────\n",
            "TEST SET RESULTS:\n",
            "──────────────────────────────────────────────────\n",
            "Average Word-Level Accuracy: 14.80%\n",
            "──────────────────────────────────────────────────\n"
          ]
        }
      ]
    }
  ]
}